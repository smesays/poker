{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/6000 (0%)]\tLoss: 1.798274\n",
      "Train Epoch: 0 [500/6000 (8%)]\tLoss: 1.406883\n",
      "Train Epoch: 0 [1000/6000 (17%)]\tLoss: 1.106637\n",
      "Train Epoch: 0 [1500/6000 (25%)]\tLoss: 1.760035\n",
      "Train Epoch: 0 [2000/6000 (33%)]\tLoss: 1.584574\n",
      "Train Epoch: 0 [2500/6000 (42%)]\tLoss: 1.074481\n",
      "Train Epoch: 0 [3000/6000 (50%)]\tLoss: 0.916654\n",
      "Train Epoch: 0 [3500/6000 (58%)]\tLoss: 0.961265\n",
      "Train Epoch: 0 [4000/6000 (67%)]\tLoss: 1.040295\n",
      "Train Epoch: 0 [4500/6000 (75%)]\tLoss: 0.985347\n",
      "Train Epoch: 0 [5000/6000 (83%)]\tLoss: 0.969723\n",
      "Train Epoch: 0 [5500/6000 (92%)]\tLoss: 1.340052\n",
      "Train Epoch: 1 [0/6000 (0%)]\tLoss: 1.308497\n",
      "Train Epoch: 1 [500/6000 (8%)]\tLoss: 1.164807\n",
      "Train Epoch: 1 [1000/6000 (17%)]\tLoss: 1.212848\n",
      "Train Epoch: 1 [1500/6000 (25%)]\tLoss: 1.415691\n",
      "Train Epoch: 1 [2000/6000 (33%)]\tLoss: 1.500311\n",
      "Train Epoch: 1 [2500/6000 (42%)]\tLoss: 0.795132\n",
      "Train Epoch: 1 [3000/6000 (50%)]\tLoss: 1.069013\n",
      "Train Epoch: 1 [3500/6000 (58%)]\tLoss: 1.211971\n",
      "Train Epoch: 1 [4000/6000 (67%)]\tLoss: 1.420276\n",
      "Train Epoch: 1 [4500/6000 (75%)]\tLoss: 1.538244\n",
      "Train Epoch: 1 [5000/6000 (83%)]\tLoss: 1.456172\n",
      "Train Epoch: 1 [5500/6000 (92%)]\tLoss: 1.107813\n",
      "Train Epoch: 2 [0/6000 (0%)]\tLoss: 1.716020\n",
      "Train Epoch: 2 [500/6000 (8%)]\tLoss: 1.605986\n",
      "Train Epoch: 2 [1000/6000 (17%)]\tLoss: 1.131749\n",
      "Train Epoch: 2 [1500/6000 (25%)]\tLoss: 1.654008\n",
      "Train Epoch: 2 [2000/6000 (33%)]\tLoss: 1.208130\n",
      "Train Epoch: 2 [2500/6000 (42%)]\tLoss: 1.397876\n",
      "Train Epoch: 2 [3000/6000 (50%)]\tLoss: 1.455207\n",
      "Train Epoch: 2 [3500/6000 (58%)]\tLoss: 1.536039\n",
      "Train Epoch: 2 [4000/6000 (67%)]\tLoss: 1.379762\n",
      "Train Epoch: 2 [4500/6000 (75%)]\tLoss: 1.501398\n",
      "Train Epoch: 2 [5000/6000 (83%)]\tLoss: 1.203251\n",
      "Train Epoch: 2 [5500/6000 (92%)]\tLoss: 1.533930\n",
      "Train Epoch: 3 [0/6000 (0%)]\tLoss: 1.161278\n",
      "Train Epoch: 3 [500/6000 (8%)]\tLoss: 0.980103\n",
      "Train Epoch: 3 [1000/6000 (17%)]\tLoss: 1.587430\n",
      "Train Epoch: 3 [1500/6000 (25%)]\tLoss: 1.261850\n",
      "Train Epoch: 3 [2000/6000 (33%)]\tLoss: 1.518443\n",
      "Train Epoch: 3 [2500/6000 (42%)]\tLoss: 0.828301\n",
      "Train Epoch: 3 [3000/6000 (50%)]\tLoss: 1.167408\n",
      "Train Epoch: 3 [3500/6000 (58%)]\tLoss: 1.241377\n",
      "Train Epoch: 3 [4000/6000 (67%)]\tLoss: 1.317270\n",
      "Train Epoch: 3 [4500/6000 (75%)]\tLoss: 1.377982\n",
      "Train Epoch: 3 [5000/6000 (83%)]\tLoss: 1.288510\n",
      "Train Epoch: 3 [5500/6000 (92%)]\tLoss: 1.471506\n",
      "Train Epoch: 4 [0/6000 (0%)]\tLoss: 1.349240\n",
      "Train Epoch: 4 [500/6000 (8%)]\tLoss: 0.999342\n",
      "Train Epoch: 4 [1000/6000 (17%)]\tLoss: 1.150095\n",
      "Train Epoch: 4 [1500/6000 (25%)]\tLoss: 1.067981\n",
      "Train Epoch: 4 [2000/6000 (33%)]\tLoss: 0.836144\n",
      "Train Epoch: 4 [2500/6000 (42%)]\tLoss: 1.295090\n",
      "Train Epoch: 4 [3000/6000 (50%)]\tLoss: 1.391861\n",
      "Train Epoch: 4 [3500/6000 (58%)]\tLoss: 1.257261\n",
      "Train Epoch: 4 [4000/6000 (67%)]\tLoss: 0.834391\n",
      "Train Epoch: 4 [4500/6000 (75%)]\tLoss: 0.862212\n",
      "Train Epoch: 4 [5000/6000 (83%)]\tLoss: 1.055756\n",
      "Train Epoch: 4 [5500/6000 (92%)]\tLoss: 1.066877\n",
      "Train Epoch: 5 [0/6000 (0%)]\tLoss: 1.303898\n",
      "Train Epoch: 5 [500/6000 (8%)]\tLoss: 1.298491\n",
      "Train Epoch: 5 [1000/6000 (17%)]\tLoss: 1.332805\n",
      "Train Epoch: 5 [1500/6000 (25%)]\tLoss: 1.208174\n",
      "Train Epoch: 5 [2000/6000 (33%)]\tLoss: 1.170883\n",
      "Train Epoch: 5 [2500/6000 (42%)]\tLoss: 1.236425\n",
      "Train Epoch: 5 [3000/6000 (50%)]\tLoss: 1.475933\n",
      "Train Epoch: 5 [3500/6000 (58%)]\tLoss: 1.346482\n",
      "Train Epoch: 5 [4000/6000 (67%)]\tLoss: 1.242449\n",
      "Train Epoch: 5 [4500/6000 (75%)]\tLoss: 1.197851\n",
      "Train Epoch: 5 [5000/6000 (83%)]\tLoss: 1.503356\n",
      "Train Epoch: 5 [5500/6000 (92%)]\tLoss: 1.089781\n",
      "Train Epoch: 6 [0/6000 (0%)]\tLoss: 1.211627\n",
      "Train Epoch: 6 [500/6000 (8%)]\tLoss: 1.521204\n",
      "Train Epoch: 6 [1000/6000 (17%)]\tLoss: 1.400161\n",
      "Train Epoch: 6 [1500/6000 (25%)]\tLoss: 1.713041\n",
      "Train Epoch: 6 [2000/6000 (33%)]\tLoss: 1.218604\n",
      "Train Epoch: 6 [2500/6000 (42%)]\tLoss: 0.993407\n",
      "Train Epoch: 6 [3000/6000 (50%)]\tLoss: 1.091582\n",
      "Train Epoch: 6 [3500/6000 (58%)]\tLoss: 1.141315\n",
      "Train Epoch: 6 [4000/6000 (67%)]\tLoss: 1.136510\n",
      "Train Epoch: 6 [4500/6000 (75%)]\tLoss: 1.257920\n",
      "Train Epoch: 6 [5000/6000 (83%)]\tLoss: 1.321609\n",
      "Train Epoch: 6 [5500/6000 (92%)]\tLoss: 1.365267\n",
      "Train Epoch: 7 [0/6000 (0%)]\tLoss: 0.829804\n",
      "Train Epoch: 7 [500/6000 (8%)]\tLoss: 1.316528\n",
      "Train Epoch: 7 [1000/6000 (17%)]\tLoss: 1.241207\n",
      "Train Epoch: 7 [1500/6000 (25%)]\tLoss: 1.162755\n",
      "Train Epoch: 7 [2000/6000 (33%)]\tLoss: 0.743004\n",
      "Train Epoch: 7 [2500/6000 (42%)]\tLoss: 1.292309\n",
      "Train Epoch: 7 [3000/6000 (50%)]\tLoss: 1.019471\n",
      "Train Epoch: 7 [3500/6000 (58%)]\tLoss: 1.311722\n",
      "Train Epoch: 7 [4000/6000 (67%)]\tLoss: 1.269049\n",
      "Train Epoch: 7 [4500/6000 (75%)]\tLoss: 1.327351\n",
      "Train Epoch: 7 [5000/6000 (83%)]\tLoss: 1.042105\n",
      "Train Epoch: 7 [5500/6000 (92%)]\tLoss: 1.131771\n",
      "Train Epoch: 8 [0/6000 (0%)]\tLoss: 1.264503\n",
      "Train Epoch: 8 [500/6000 (8%)]\tLoss: 1.269944\n",
      "Train Epoch: 8 [1000/6000 (17%)]\tLoss: 1.145223\n",
      "Train Epoch: 8 [1500/6000 (25%)]\tLoss: 1.299126\n",
      "Train Epoch: 8 [2000/6000 (33%)]\tLoss: 1.116562\n",
      "Train Epoch: 8 [2500/6000 (42%)]\tLoss: 1.074928\n",
      "Train Epoch: 8 [3000/6000 (50%)]\tLoss: 1.197404\n",
      "Train Epoch: 8 [3500/6000 (58%)]\tLoss: 0.990878\n",
      "Train Epoch: 8 [4000/6000 (67%)]\tLoss: 1.437852\n",
      "Train Epoch: 8 [4500/6000 (75%)]\tLoss: 1.447027\n",
      "Train Epoch: 8 [5000/6000 (83%)]\tLoss: 1.496546\n",
      "Train Epoch: 8 [5500/6000 (92%)]\tLoss: 1.053053\n",
      "Train Epoch: 9 [0/6000 (0%)]\tLoss: 1.330422\n",
      "Train Epoch: 9 [500/6000 (8%)]\tLoss: 1.178022\n",
      "Train Epoch: 9 [1000/6000 (17%)]\tLoss: 1.262442\n",
      "Train Epoch: 9 [1500/6000 (25%)]\tLoss: 1.260105\n",
      "Train Epoch: 9 [2000/6000 (33%)]\tLoss: 0.852396\n",
      "Train Epoch: 9 [2500/6000 (42%)]\tLoss: 0.933747\n",
      "Train Epoch: 9 [3000/6000 (50%)]\tLoss: 1.605101\n",
      "Train Epoch: 9 [3500/6000 (58%)]\tLoss: 1.336567\n",
      "Train Epoch: 9 [4000/6000 (67%)]\tLoss: 1.438063\n",
      "Train Epoch: 9 [4500/6000 (75%)]\tLoss: 1.526687\n",
      "Train Epoch: 9 [5000/6000 (83%)]\tLoss: 1.403963\n",
      "Train Epoch: 9 [5500/6000 (92%)]\tLoss: 1.142189\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Global Parameters:\n",
    "BATCH_SIZE = 10\n",
    "IS_CUDA    = False\n",
    "LR         = 0.03\n",
    "MOMENTUM   = 1e-6\n",
    "EPOCHS      = 10\n",
    "LOG_IN     = 50\n",
    "\n",
    "# Load Dataset\n",
    "def load_dataset(txt_filepath):\n",
    "    \n",
    "    # PLace to load the informaton we have\n",
    "    #features = np.loadtxt(txt_filepath).T\n",
    "    data = []\n",
    "    target = []\n",
    "    for i in range(1,7):\n",
    "        for _ in range(100):\n",
    "            data.append(np.random.randint(5*i, size=19))\n",
    "            target.append(np.random.randint(i))\n",
    "    return np.array(data), np.array(target)\n",
    "\n",
    "# Global objects:\n",
    "class PokerDataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_filepath):\n",
    "        self.data, self.target = load_dataset(txt_filepath)\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index,:], self.target[index] \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "# Network Architecture:\n",
    "class BlueNet_all(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BlueNet_all, self).__init__()\n",
    "        self.layer1 = nn.Linear(20, 40)\n",
    "        self.layer2 = nn.Linear(40, 120)\n",
    "        self.layer3 = nn.Linear(120, 60)\n",
    "        self.layer4 = nn.Linear(60, 30)\n",
    "        self.layer5 = nn.Linear(30, 6)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.relu(self.layer2(self.relu((self.layer1(x)))))\n",
    "        out_x = self.softplus(self.layer5(self.relu(self.layer4(self.relu((self.layer3(fc1)))))))\n",
    "        return out_x\n",
    "    \n",
    "class BlueNet_Autoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BlueNet_v2, self).__init__()\n",
    "        self.layer1 = nn.Linear(19, 10)\n",
    "        self.layer2 = nn.Linear(10, 6)\n",
    "        self.layer3 = nn.Linear(6, 10)\n",
    "        self.layer4 = nn.Linear(10, 19)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softplus =  nn.Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        fc1 = self.relu(self.layer2(self.relu((self.layer1(x)))))\n",
    "        out_x = self.softplus(self.layer5(self.relu(self.layer4(self.relu((self.layer3(fc1)))))))\n",
    "        return out_x\n",
    "\n",
    "train_set = PokerDataset(0)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "model = BlueNet_v1()\n",
    "if IS_CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=MOMENTUM)\n",
    "\n",
    "# Use MSE as Reconstrcution Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # reshape to vector\n",
    "        #data = data.view(data.size(0), -1)\n",
    "        # move to cuda if available\n",
    "        if IS_CUDA:\n",
    "            data = data.cuda()\n",
    "        # convert to Variable\n",
    "        data = Variable(data.type(torch.FloatTensor))\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        # forward: evaluate with model\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        # backward: compute gradient and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_IN == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "best_acc = 0\n",
    "# run training\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)\n",
    "    #test_loss = test(show_plot=epoch%10 == 1)\n",
    "    #if test_loss < best_loss:\n",
    "    #    best_loss = test_loss\n",
    "    #    if args.is_noisy:\n",
    "    #        torch.save(model.state_dict(), 'best_fc_denoising_autoencoder.pth')\n",
    "    #    else:\n",
    "    #        torch.save(model.state_dict(), 'best_fc_autoencoder.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
